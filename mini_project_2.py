# -*- coding: utf-8 -*-
"""Mini project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBiq1A_f61N9T9l0VZwQ880JgK2EkJbY

# **Mini Project 2 Done by: Rahma Nouaji,  Mohammad Ghavidel, Bita Farokhian**

---

# Task 1: Dataset preprocessing

Import libraries
"""

import numpy as np # linear algebra
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
import tensorflow as tf

"""
Load the CIFAR-10 dataset


"""

from keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

"""Exploring the dataset

"""

##Class Distribution
# Count the number of images in each class
unique, counts = np.unique(y_train, return_counts=True)
class_count = dict(zip(unique, counts))

# Plot a bar chart of the class distribution
plt.bar(class_count.keys(), class_count.values())
plt.xticks(range(10), ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], rotation=90)
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

print("Shape of training data:")
print(x_train.shape)
print(y_train.shape)
print("Shape of test data:")
print(x_test.shape)
print(y_test.shape)

# Plot the first 20 images from the training set
fig, axes = plt.subplots(4, 5, figsize=(20, 5))
axes = axes.ravel()
for i in range(20):
    axes[i].imshow(x_train[i])
    axes[i].set_title(y_train[i])
    axes[i].axis('off')
plt.show()

# Display the first 10 images from each class
fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15,15))
for i in range(10):
    images = x_train[y_train.flatten() == i][:10]
    for j in range(10):
        axes[i][j].imshow(images[j])
        axes[i][j].axis('off')
plt.subplots_adjust(wspace=0.1, hspace=0.1)
plt.show()

# Flatten the images and plot a histogram of the pixel values
pixels = x_train.flatten()
plt.hist(pixels, bins=256, range=(0,255))
plt.title('Pixel Value Distribution')
plt.xlabel('Pixel Value')
plt.ylabel('Count')
plt.show()

"""> The resulting chart shows a roughly uniform distribution of pixel values between 0 and 255, indicating that the images in the dataset are not too bright or too dark.

Preprocessing of the Dataset
"""

# Transform label indices to one-hot encoded vectors

#y_train = to_categorical(y_train, num_classes=10)
#y_test = to_categorical(y_test, num_classes=10)

# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)

#X_train = np.reshape(x_train,(50000,3072))
#X_test = np.reshape(x_test,(10000,3072))
#X_train = X_train.astype('float32')
#X_test = X_test.astype('float32')

# Compute mean and standard deviation of the pixel values
mean = np.mean(x_train, axis=(0,1,2,3))
std = np.std(x_train, axis=(0,1,2,3))

# Transform label indices to one-hot encoded vectors

y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Center and normalize the data
x_train = (x_train )/255
x_test = (x_test )/255

# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)

X_train = np.reshape(x_train,(50000,3072))
X_test = np.reshape(x_test,(10000,3072))

"""#Task 2: MLP model"""

#Activation functions Tanh
def tanh(x):
 y= 2/(1 + np.exp(-2*x)) - 1
 return y
#activation_tanh = {(lambda x: np.tanh(x))}
deriv_tanh = {(lambda x: 1-x**2)}
#Activation function ReLU
activation_ReLU = {(lambda x: x*(x > 0))}
deriv_ReLU = {(lambda x: 1 * (x>0))}
#Activation function leaky ReLU
leaky_Relu= {(lambda x: x*0.01 if x < 0 else x)}
derive_leaky_Relu= {(lambda x: 0.01 if x < 0 else 1)}
#Activation function Sigmoid
activation_sigmoid= {(lambda x: 1 / (1 + np.exp(-x)))}
##Derivative sigmoid
def derivative_sigmoid(x):
        s = activation_sigmoid(x)
        return s * (1 - s)
activation_softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0,keepdims=True)

deriv_softmax = lambda x: np.diag(activation_softmax(x)) - np.outer(activation_softmax(x), activation_softmax(x))

class Layer:
    def __init__(self, units=None, input_units=None, activation=None, l2_regularization=0):
            self.input_units = input_units
            self.output_units = units
            self.regularization = l2_regularization
            assert activation in [None,'tanh','leaky_relu','relu','sigmoid','softmax'], 'Activation not recognized'
            self.activation = activation

class Model:
    def __init__(self, layers, batch_size,lr,max_norm=None,random_state=21):

        np.random.seed(random_state)
        self.batch_size=batch_size
        self.lr=lr
        self.parameters = {}
        self.activations = {}
        self.gradients = {}
        self.max_norm=max_norm
        self.regularization ={}
        self.cache = {}
        self.X = None
        self.Y = None
        self.cost_train=[]
        self.cost_vali=[]
        self.train_accuracy = []
        self.val_accuracy = []

        self.L = len(layers)

        for l in range(1,self.L+1):

            layer = layers[l-1]

            if layer.input_units:
                input_units = layer.input_units
                output_units = layer.output_units
            else:
                input_units  = previous_output_units
                output_units = layer.output_units

            # Apply Xavier Initialization
            #xavier_stddev = np.sqrt(2 / (input_units + output_units))
            self.parameters['W' + str(l)] = np.random.randn(output_units,input_units) * np.sqrt(2./input_units)
            self.parameters['b' + str(l)] = np.zeros((layer.output_units,1))

            self.activations['a' + str(l)] = layer.activation
            self.regularization['r' + str(l)] = layer.regularization

            previous_output_units = layer.output_units

    def forward(self, X):
      A_prev = X
      for l in range(1, self.L+1):
        Wl = self.parameters['W' + str(l)]
        bl = self.parameters['b' + str(l)]
        activation_fn = self.activations['a' + str(l)]

        Zl = np.dot(Wl, A_prev) + bl
        if activation_fn == 'tanh':
            Al = tanh(Zl)
        elif activation_fn == 'relu':
            Al = np.maximum(0, Zl)
        elif activation_fn == 'sigmoid':
            Al = 1 / (1 + np.exp(-Zl))
        elif activation_fn == 'softmax':
            exp_Zl = np.exp(Zl)
            Al = exp_Zl / np.sum(exp_Zl, axis=0, keepdims=True)
        elif activation_fn == 'leaky_relu':
            Al = np.maximum(0.01 * Zl, Zl)

        self.cache['Z' + str(l)] = Zl
        self.cache['A' + str(l)] = Al

        A_prev = Al

      return Al




    def loss(self, AL, Y):
        m = Y.shape[1]
        cost=0


        loss_each_example = -np.sum(Y * np.log(AL),axis=1)
        all_losses = np.sum(loss_each_example)
        cost = all_losses/m
        return cost
    def accuracy(self, A, Y):
        # calculate accuracy here

        predictions = np.argmax(A, axis=0)
        labels = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == labels)
        return accuracy

    def backward(self):
      m = self.X.shape[1] # number of examples

      # Retrieve all weights, biases, activations, and regularization parameters
      W, b, a, r = {}, {}, {}, {}
      A, Z = {}, {}
      for l in range(1, self.L+1):
          W[l] = self.parameters['W' + str(l)]
          b[l] = self.parameters['b' + str(l)]
          a[l] = self.activations['a' + str(l)]
          r[l] = self.regularization['r' + str(l)]

          A[l] = self.cache['A' + str(l)]
          Z[l] = self.cache['Z' + str(l)]
      A[0] = self.X

      # Backward propagation: calculate dW1, db1, ..., dWL, dbL
      dW, db, dZ = {}, {}, {}
      for l in reversed(range(1, self.L+1)):
          if a[l] == 'sigmoid' or a[l] == 'softmax':
              dZ[l] = A[l] - self.Y
          elif a[l] == 'tanh':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (1 - np.square(Z[l])))
          elif a[l] == 'relu':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0))
          elif a[l] == 'leaky_relu':
            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0) + (Z[l] <= 0) * 0.01)
          dW[l] = np.dot(dZ[l], A[l-1].T) / m
          db[l] = np.sum(dZ[l], axis=1, keepdims=True) / m
         # if self.max_norm:
          #  for l in reversed(range(1, self.L+1)):
           #   dW[l] = np.clip(dW[l], a_min=-self.max_norm, a_max=self.max_norm)
            #  db[l] = np.clip(db[l], a_min=-self.max_norm, a_max=self.max_norm)

      # Update weights and biases using gradient descent
      for l in range(1, self.L+1):
          W[l] -= self.lr * dW[l]
          b[l] -= self.lr * db[l]
          self.parameters['W' + str(l)] = W[l]
          self.parameters['b' + str(l)] = b[l]


    def get_batch(self, X, Y):

        np.random.seed(1)
        m = X.shape[1]
        mini_batches = []

        #shuffling the data
        permutation = list(np.random.permutation(X.shape[1]))
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]
        num_of_complete_batches = m // self.batch_size
        for i in range(num_of_complete_batches):
            mini_batch_X = X_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch_Y = Y_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)

        #if there is incomplete batch
        if m % self.batch_size!= 0:

            mini_batch_X=X_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch_Y=Y_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    def fit(self,
            X,
            Y,
            validation_data=None,
            epochs=100,
            patience=None):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values
        if isinstance(Y, (pd.core.frame.DataFrame,pd.core.series.Series)):
            Y = Y.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        if Y.ndim > 1:
            Y = Y.T
        else:
            Y = np.expand_dims(Y, axis=0)

        self.full_Y = Y
        self.full_X = X

        mini_batches = self.get_batch(X, Y)

        if validation_data:
            X_val, Y_val = validation_data

            if isinstance(X_val, pd.core.frame.DataFrame):
                X_val = X_val.values
            if isinstance(Y_val, (pd.core.frame.DataFrame,pd.core.series.Series)):
                Y_val = Y_val.values

            if X_val.ndim > 1:
                X_val = X_val.T
            else:
                X_val = np.expand_dims(X_val, axis=1)

            if Y_val.ndim > 1:
                Y_val = Y_val.T
            else:
                Y_val = np.expand_dims(Y_val, axis=0)

        if patience:
            epoch_cache ={}
        cost_train=[]
        cost_vali=[]
        for i in range(0, epochs):

            for mini_batch in mini_batches:

                mini_batch_X, mini_batch_Y = mini_batch

                self.X = mini_batch_X
                self.Y = mini_batch_Y

                AL = self.forward(mini_batch_X)
                cost = self.loss(AL, mini_batch_Y)
                train_acc = self.accuracy(AL, mini_batch_Y)



                self.backward()

            if validation_data:
                AL_val = self.forward(X_val)
                cost_val = self.loss(AL_val, Y_val)

            # Print the cost and acc every epoch
            if validation_data:
                train_acc = self.accuracy(AL, mini_batch_Y)
                val_acc = self.accuracy(AL_val, Y_val)
                self.val_accuracy.append(val_acc)
                print(f"Epoch {i+1} - Train_Loss:{cost}  Val_Loss:{cost_val} Train_Acc:{train_acc} Val_Acc:{val_acc}")
                self.cost_train.append(cost)
                self.train_accuracy.append(train_acc)
                self.cost_vali.append(cost_val)

            else:

                self.cost_train.append(cost)
                print(f"Epoch {i+1} - Train_Loss:{cost}")

            if patience:
               if not epoch_cache or cost_val <= epoch_cache['best_loss']:
                    epoch_cache['best_loss'] = cost_val
                    epoch_cache['best_parameters'] = self.parameters
                    counter=0
               elif cost_val > epoch_cache['best_loss']:
                    counter += 1

               if counter == patience:
                    self.parameters = epoch_cache['best_parameters']
                    print(f"Early Stopping! Best Epoch {i+1-patience} - Val_Loss:{epoch_cache['best_loss']}")
                    break


    def predict(self, X):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        AL = self.forward(X)

        if AL.shape[0]==1:
            y_pred = AL[0]>0.5
        else:
            y_pred = (AL == np.max(AL,axis=0)).astype(bool).T
        y_pred = y_pred*1
        y_pred. astype('float32')
        return y_pred


    def get_cost(self,validation=True):
      if validation:
          return self.cost_train,self.cost_vali
      else:
        return self.cost_train
    def score(self,validation=True):
      if validation:
        return self.train_accuracy, self.val_accuracy
      else:
        return self.train_accuracy

def evaluate_acc(y, y_pred):
  acc=np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))
  print('The evaluation accuracy is ', acc)

# tran val split 25%
X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,random_state=1,test_size=0.25)

paramters=[Layer(units=256,input_units=3072, activation='relu'),
               Layer(units=64, activation='relu'),
               Layer(units=10, activation='softmax')]
model = Model(paramters, batch_size=64,lr=0.001,
               random_state=4)

model.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),
          epochs=30)

plt.plot(model.get_cost()[0],label='Training Loss')
plt.plot(model.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(model.score()[0],label='Training acc')
plt.plot(model.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

pred = np.where(model.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

y_pred=model.predict(X_test)

evaluate_acc(y_test,y_pred)

"""##Gridsearch cross validation from scratch"""

from sklearn.model_selection import KFold
import numpy as np
parameters=[Layer(units=256,input_units=3072, activation='relu'),
               Layer(units=256, activation='relu'),
               Layer(units=10, activation='softmax')]
def train_evaluate_model(params, X_train, y_train, X_val, y_val, num_epochs=30,patience=5):
    model = Model(layers=parameters, lr=params['lr'],
                  batch_size=params['batch_size'], random_state=1)
    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)
    val_error =model.get_cost()[1]
    return val_error


# Define hyperparameters and their possible values
params_grid = {

    'lr': [0.001, 0.01, 0.1],
    'batch_size': [128, 64, 32],
}

# Define number of folds for cross-validation
num_folds = 5

# Create list of hyperparameter combinations to test
param_combinations = []

for lr in params_grid['lr']:
    for batch_size in params_grid['batch_size']:
            params = { 'lr': lr, 'batch_size': batch_size}
            param_combinations.append(params)

# Perform cross-validation on each set of hyperparameters
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
results = {}
for params in param_combinations:
    val_errors = []
    for train_idx, val_idx in kf.split(X_train):
        # split data into training and validation sets for current fold
        X_train_fold = X_train[train_idx]
        y_train_fold = y_train[train_idx]
        X_val_fold = X_train[val_idx]
        y_val_fold = y_train[val_idx]

        # train and evaluate model on current fold
        val_error = train_evaluate_model(params, X_train_fold, y_train_fold, X_val_fold, y_val_fold)

        # append validation accuracy to list of validation accuracies for this set of hyperparameters
        val_errors.append(val_error)
     # calculate mean and standard deviation of validation accuracies for this set of hyperparameters
    mean_val_error = np.mean(val_errors)
    std_val_error = np.std(val_errors)

    # print results for this set of hyperparameters
    print("Validation error for this set of hyperparameters {param} is {valerror}".format(param=params,valerror= val_errors))
    print("Mean validation error for this set of hyperparameters {param} is {meanval}".format(param=params,meanval= mean_val_error))
    print("Standard deviation of validation error for this set of hyperparameters {param} is {stderr}".format(param=params,stderr= std_val_error))

    # update dictionary of results with mean validation accuracy for this set of hyperparameters
    results[str(params)] = [mean_val_error,std_val_error]

# print final results
print("Final results:", results)

#results= {"{'lr': 0.001, 'batch_size': 128}": [1.8796730562788306, 0.09944853792979212], "{'lr': 0.001, 'batch_size': 64}": [1.7998515756756002, 0.09945365315766191], "{'lr': 0.001, 'batch_size': 32}": [1.732251848092999, 0.10083217523854918], "{'lr': 0.01, 'batch_size': 128}": [1.6327004204378697, 0.10586798932949268], "{'lr': 0.01, 'batch_size': 64}": [1.6112549625284995, 0.1025109540314804], "{'lr': 0.01, 'batch_size': 32}": [1.7039695269572208, 0.1674026532482218], "{'lr': 0.1, 'batch_size': 128}": [1.6795054702972405, 0.10283320410060352], "{'lr': 0.1, 'batch_size': 64}": [1.76709603969979, 0.13654029209921129], "{'lr': 0.1, 'batch_size': 32}": [2.0538286117707774, 0.30143014484522224]}

import pandas as pd
# Data is the same as final results
# Create another dictionary from the final result dictionary


# Create a DataFrame from the dictionary
df = pd.DataFrame.from_dict(results, orient='index', columns=['Mean val error', 'Std of val error'])

# Extract the hyperparameters from the index
df.index = df.index.map(eval)

# Reset the index to create separate columns for the hyperparameters
df = df.reset_index().rename(columns={'index': 'Hyperparameters'})

# Print the resulting DataFrame
print(df)

# Find the row with the minimum loss
min_std_row = df.loc[df['Std of val error'].idxmin()]
min_mean_row = df.loc[df['Mean val error'].idxmin()]
# Get the corresponding hyperparameters
hyperparams = min_std_row['Hyperparameters']
hyperparams = min_mean_row['Hyperparameters']
# Print the hyperparameters
print('The hyperparameters that give the lowest std are ',hyperparams)
print('The hyperparameters that give the lowest mean are ',hyperparams)

"""# Task 3: Run the experiments

## Task 3.1: Three MLP Models
"""

##We added early stopping to prevent the model from overfitting which is enabled with the parameter patience

"""The patience parameter determines the number of epochs to wait before stopping training if there is no improvement in the validation loss. The implementation keeps track of the best loss and parameters using a cache, and if the current validation loss is better than the previous best, it updates the cache and resets the counter. If the validation loss is worse than the previous best, it increments the counter.

Once the counter reaches the patience value, the implementation rolls back to the best parameters and stops the training loop. It also prints out the best epoch and validation loss for reference.
"""

X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,random_state=1,test_size=0.25)

"""### First Model: Input to Ouput"""

Inp_out_layer_paramters=[Layer(units=10,input_units=3072, activation='softmax')]
model1 = Model(Inp_out_layer_paramters, batch_size=64,lr=0.01,
               random_state=4)

model1.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,
          epochs=30)

pred1 = np.where(model1.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred1,digits=4))

y_pred1=model1.predict(X_test)

evaluate_acc(y_test,y_pred1)

plt.plot(model1.get_cost()[0],label='Training Loss')
plt.plot(model1.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the first model')
plt.legend()
plt.show()

plt.plot(model1.score()[0],label='Training acc')
plt.plot(model1.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the first model')

plt.legend()
plt.show()

""">First model is underfitting since it has a very high training error.

### Second Model: 1 hidden layer
"""

onehidden_layer_paramters=[Layer(units=256,input_units=3072, activation='relu'),
               Layer(units=10, activation='softmax')]
model2 = Model(onehidden_layer_paramters,  batch_size=64,lr=0.01,
               random_state=1)

model2.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30
    )

pred = np.where(model2.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

y_pred=model2.predict(X_test)

evaluate_acc(y_test,y_pred)

plt.plot(model2.get_cost()[0],label='Training Loss')
plt.plot(model2.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the Second model')
plt.legend()
plt.show()

plt.plot(model2.score()[0],label='Training acc')
plt.plot(model2.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the Second model')

plt.legend()
plt.show()

"""### Third Model: 2 hidden layers"""

twohidden_layer_parameters=[Layer(units=256,input_units=3072, activation='relu'),
               Layer(units=256, activation='relu'),
               Layer(units=10, activation='softmax')]
model3 = Model(twohidden_layer_parameters,  batch_size=64,lr=0.01,
               random_state=4)

model3.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30
          )

pred = np.where(model3.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

y_pred=model3.predict(X_test)

evaluate_acc(y_test,y_pred)

plt.plot(model3.get_cost()[0],label='Training Loss')
plt.plot(model3.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model')
plt.legend()
plt.show()

plt.plot(model3.score()[0],label='Training acc')
plt.plot(model3.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the third model')

plt.legend()
plt.show()

"""## Task 3.2: Leaky Relu and tanh activation function

### Leaky RelU activation function
"""

class Layer:
    def __init__(self, units=None, input_units=None, activation=None, l2_regularization=0):
            self.input_units = input_units
            self.output_units = units
            self.regularization = l2_regularization
            assert activation in [None,'tanh','leaky_relu','relu','sigmoid','softmax'], 'Activation not recognized'
            self.activation = activation

class Model:
    def __init__(self, layers, batch_size,lr,max_norm=None,random_state=21):

        np.random.seed(random_state)
        self.batch_size=batch_size
        self.lr=lr
        self.parameters = {}
        self.activations = {}
        self.gradients = {}
        self.max_norm=max_norm
        self.regularization ={}
        self.cache = {}
        self.X = None
        self.Y = None
        self.cost_train=[]
        self.cost_vali=[]
        self.train_accuracy = []
        self.val_accuracy = []

        self.L = len(layers)

        for l in range(1,self.L+1):

            layer = layers[l-1]

            if layer.input_units:
                input_units = layer.input_units
                output_units = layer.output_units
            else:
                input_units  = previous_output_units
                output_units = layer.output_units

            # Apply Xavier Initialization
            #xavier_stddev = np.sqrt(2 / (input_units + output_units))
            self.parameters['W' + str(l)] = np.random.randn(output_units,input_units) * np.sqrt(2./input_units)
            self.parameters['b' + str(l)] = np.zeros((layer.output_units,1))

            self.activations['a' + str(l)] = layer.activation
            self.regularization['r' + str(l)] = layer.regularization

            previous_output_units = layer.output_units

    def forward(self, X):
      A_prev = X
      for l in range(1, self.L+1):
        Wl = self.parameters['W' + str(l)]
        bl = self.parameters['b' + str(l)]
        activation_fn = self.activations['a' + str(l)]

        Zl = np.dot(Wl, A_prev) + bl
        if activation_fn == 'tanh':
            Al = tanh(Zl)
        elif activation_fn == 'relu':
            Al = np.maximum(0, Zl)
        elif activation_fn == 'sigmoid':
            Al = 1 / (1 + np.exp(-Zl))
        elif activation_fn == 'softmax':
            exp_Zl = np.exp(Zl)
            Al = exp_Zl / np.sum(exp_Zl, axis=0, keepdims=True)
        elif activation_fn == 'leaky_relu':
            Al = np.maximum(0.01 * Zl, Zl)

        self.cache['Z' + str(l)] = Zl
        self.cache['A' + str(l)] = Al

        A_prev = Al

      return Al




    def loss(self, AL, Y):
        m = Y.shape[1]
        cost=0


        loss_each_example = -np.sum(Y * np.log(AL),axis=1)
        all_losses = np.sum(loss_each_example)
        cost = all_losses/m
        return cost
    def accuracy(self, A, Y):
        # calculate accuracy here

        predictions = np.argmax(A, axis=0)
        labels = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == labels)
        return accuracy

    def backward(self):
      m = self.X.shape[1] # number of examples

      # Retrieve all weights, biases, activations, and regularization parameters
      W, b, a, r = {}, {}, {}, {}
      A, Z = {}, {}
      for l in range(1, self.L+1):
          W[l] = self.parameters['W' + str(l)]
          b[l] = self.parameters['b' + str(l)]
          a[l] = self.activations['a' + str(l)]
          r[l] = self.regularization['r' + str(l)]

          A[l] = self.cache['A' + str(l)]
          Z[l] = self.cache['Z' + str(l)]
      A[0] = self.X

      # Backward propagation: calculate dW1, db1, ..., dWL, dbL
      dW, db, dZ = {}, {}, {}
      for l in reversed(range(1, self.L+1)):
          if a[l] == 'sigmoid' or a[l] == 'softmax':
              dZ[l] = A[l] - self.Y
          elif a[l] == 'tanh':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (1 - np.square(Z[l])))
          elif a[l] == 'relu':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0))
          elif a[l] == 'leaky_relu':
            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0) + (Z[l] <= 0) * 0.01)
          dW[l] = np.dot(dZ[l], A[l-1].T) / m
          db[l] = np.sum(dZ[l], axis=1, keepdims=True) / m
      if self.max_norm:
            for j in reversed(range(1, self.L+1)):
              dW[j] = np.clip(dW[j], a_min=-self.max_norm, a_max=self.max_norm)


      # Update weights and biases using gradient descent
      for l in range(1, self.L+1):
          W[l] -= self.lr * dW[l]
          b[l] -= self.lr * db[l]
          self.parameters['W' + str(l)] = W[l]
          self.parameters['b' + str(l)] = b[l]


    def get_batch(self, X, Y):

        np.random.seed(1)
        m = X.shape[1]
        mini_batches = []

        #shuffling the data
        permutation = list(np.random.permutation(X.shape[1]))
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]
        num_of_complete_batches = m // self.batch_size
        for i in range(num_of_complete_batches):
            mini_batch_X = X_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch_Y = Y_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)

        #if there is incomplete batch
        if m % self.batch_size!= 0:

            mini_batch_X=X_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch_Y=Y_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    def fit(self,
            X,
            Y,
            validation_data=None,
            epochs=100,
            patience=None):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values
        if isinstance(Y, (pd.core.frame.DataFrame,pd.core.series.Series)):
            Y = Y.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        if Y.ndim > 1:
            Y = Y.T
        else:
            Y = np.expand_dims(Y, axis=0)

        self.full_Y = Y
        self.full_X = X

        mini_batches = self.get_batch(X, Y)

        if validation_data:
            X_val, Y_val = validation_data

            if isinstance(X_val, pd.core.frame.DataFrame):
                X_val = X_val.values
            if isinstance(Y_val, (pd.core.frame.DataFrame,pd.core.series.Series)):
                Y_val = Y_val.values

            if X_val.ndim > 1:
                X_val = X_val.T
            else:
                X_val = np.expand_dims(X_val, axis=1)

            if Y_val.ndim > 1:
                Y_val = Y_val.T
            else:
                Y_val = np.expand_dims(Y_val, axis=0)

        if patience:
            epoch_cache ={}
        cost_train=[]
        cost_vali=[]
        for i in range(0, epochs):

            for mini_batch in mini_batches:

                mini_batch_X, mini_batch_Y = mini_batch

                self.X = mini_batch_X
                self.Y = mini_batch_Y

                AL = self.forward(mini_batch_X)
                cost = self.loss(AL, mini_batch_Y)
                train_acc = self.accuracy(AL, mini_batch_Y)



                self.backward()

            if validation_data:
                AL_val = self.forward(X_val)
                cost_val = self.loss(AL_val, Y_val)

            # Print the cost and acc every epoch
            if validation_data:
                train_acc = self.accuracy(AL, mini_batch_Y)
                val_acc = self.accuracy(AL_val, Y_val)
                self.val_accuracy.append(val_acc)
                print(f"Epoch {i+1} - Train_Loss:{cost}  Val_Loss:{cost_val} Train_Acc:{train_acc} Val_Acc:{val_acc}")
                self.cost_train.append(cost)
                self.train_accuracy.append(train_acc)
                self.cost_vali.append(cost_val)

            else:

                self.cost_train.append(cost)
                print(f"Epoch {i+1} - Train_Loss:{cost}")

            if patience:
               if not epoch_cache or cost_val <= epoch_cache['best_loss']:
                    epoch_cache['best_loss'] = cost_val
                    epoch_cache['best_parameters'] = self.parameters
                    counter=0
               elif cost_val > epoch_cache['best_loss']:
                    counter += 1

               if counter == patience:
                    self.parameters = epoch_cache['best_parameters']
                    print(f"Early Stopping! Best Epoch {i+1-patience} - Val_Loss:{epoch_cache['best_loss']}")
                    break


    def predict(self, X):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        AL = self.forward(X)

        if AL.shape[0]==1:
            y_pred = AL[0]>0.5
        else:
            y_pred = (AL == np.max(AL,axis=0)).astype(bool).T
        y_pred = y_pred*1
        y_pred. astype('float32')
        return y_pred


    def get_cost(self,validation=True):
      if validation:
          return self.cost_train,self.cost_vali
      else:
        return self.cost_train
    def score(self,validation=True):
      if validation:
        return self.train_accuracy, self.val_accuracy
      else:
        return self.train_accuracy

def evaluate_acc(y, y_pred):
  acc=np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))
  print('The evaluation accuracy is ', acc)

model3_leaky_relu= Model([Layer(units=256,input_units=3072, activation='leaky_relu'),
                Layer(units=256, activation='leaky_relu'),

               Layer(units=10, activation='softmax')],batch_size=64,lr=0.01,
               random_state=1)

model3_leaky_relu.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30
          )

y_pred=model3_leaky_relu.predict(X_test)

evaluate_acc(y_test,y_pred)

pred = np.where(model3_leaky_relu.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

plt.plot(model3_leaky_relu.score()[0],label='Training acc')
plt.plot(model3_leaky_relu.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the third model with leaky RelU alpha=0.01')
plt.legend()
plt.show()

plt.plot(model3_leaky_relu.get_cost()[0],label='Training Loss')
plt.plot(model3_leaky_relu.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model with leaky RelU alpha=0.01')
plt.legend()
plt.show()

"""### Tanh Activation function"""

model3_tanh = Model([Layer(units=256,input_units=3072, activation='tanh'),
                Layer(units=256, activation='tanh'),

               Layer(units=10, activation='softmax')],batch_size=64,lr=0.01,
               random_state=1)

model3_tanh.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),

          epochs=30
          )

"""We wanted to try max norm to enforce an upper bound to the weights so they will not explode however we had the same problem of vanishing gradient descent with tanh."""

class Layer:
    def __init__(self, units=None, input_units=None, activation=None, l2_regularization=0):
            self.input_units = input_units
            self.output_units = units
            self.regularization = l2_regularization
            assert activation in [None,'tanh','leaky_relu','relu','sigmoid','softmax'], 'Activation not recognized'
            self.activation = activation

class Model:
    def __init__(self, layers, batch_size,lr,max_norm=None,random_state=21):

        np.random.seed(random_state)
        self.batch_size=batch_size
        self.lr=lr
        self.parameters = {}
        self.activations = {}
        self.gradients = {}
        self.max_norm=max_norm
        self.regularization ={}
        self.cache = {}
        self.X = None
        self.Y = None
        self.cost_train=[]
        self.cost_vali=[]
        self.train_accuracy = []
        self.val_accuracy = []

        self.L = len(layers)

        for l in range(1,self.L+1):

            layer = layers[l-1]

            if layer.input_units:
                input_units = layer.input_units
                output_units = layer.output_units
            else:
                input_units  = previous_output_units
                output_units = layer.output_units

            # Apply Xavier Initialization
            #xavier_stddev = np.sqrt(2 / (input_units + output_units))
            self.parameters['W' + str(l)] = np.random.randn(output_units,input_units) * np.sqrt(2./input_units)
            self.parameters['b' + str(l)] = np.zeros((layer.output_units,1))

            self.activations['a' + str(l)] = layer.activation
            self.regularization['r' + str(l)] = layer.regularization

            previous_output_units = layer.output_units

    def forward(self, X):
      A_prev = X
      for l in range(1, self.L+1):
        Wl = self.parameters['W' + str(l)]
        bl = self.parameters['b' + str(l)]
        activation_fn = self.activations['a' + str(l)]

        Zl = np.dot(Wl, A_prev) + bl
        if activation_fn == 'tanh':
            Al = tanh(Zl)
        elif activation_fn == 'relu':
            Al = np.maximum(0, Zl)
        elif activation_fn == 'sigmoid':
            Al = 1 / (1 + np.exp(-Zl))
        elif activation_fn == 'softmax':
            exp_Zl = np.exp(Zl)
            Al = exp_Zl / np.sum(exp_Zl, axis=0, keepdims=True)
        elif activation_fn == 'leaky_relu':
            Al = np.maximum(0.01 * Zl, Zl)

        self.cache['Z' + str(l)] = Zl
        self.cache['A' + str(l)] = Al

        A_prev = Al

      return Al




    def loss(self, AL, Y):
        m = Y.shape[1]
        cost=0


        loss_each_example = -np.sum(Y * np.log(AL),axis=1)
        all_losses = np.sum(loss_each_example)
        cost = all_losses/m
        return cost
    def accuracy(self, A, Y):
        # calculate accuracy here

        predictions = np.argmax(A, axis=0)
        labels = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == labels)
        return accuracy

    def backward(self):
      m = self.X.shape[1] # number of examples

      # Retrieve all weights, biases, activations, and regularization parameters
      W, b, a, r = {}, {}, {}, {}
      A, Z = {}, {}
      for l in range(1, self.L+1):
          W[l] = self.parameters['W' + str(l)]
          b[l] = self.parameters['b' + str(l)]
          a[l] = self.activations['a' + str(l)]
          r[l] = self.regularization['r' + str(l)]

          A[l] = self.cache['A' + str(l)]
          Z[l] = self.cache['Z' + str(l)]
      A[0] = self.X

      # Backward propagation: calculate dW1, db1, ..., dWL, dbL
      dW, db, dZ = {}, {}, {}
      for l in reversed(range(1, self.L+1)):
          if a[l] == 'sigmoid' or a[l] == 'softmax':
              dZ[l] = A[l] - self.Y
          elif a[l] == 'tanh':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (1 - np.square(Z[l])))
          elif a[l] == 'relu':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0))
          elif a[l] == 'leaky_relu':
            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0) + (Z[l] <= 0) * 0.01)
          dW[l] = np.dot(dZ[l], A[l-1].T) / m
          db[l] = np.sum(dZ[l], axis=1, keepdims=True) / m
      if self.max_norm:
            for j in reversed(range(1, self.L+1)):
              dW[j] = np.clip(dW[j], a_min=-self.max_norm, a_max=self.max_norm)


      # Update weights and biases using gradient descent
      for l in range(1, self.L+1):
          W[l] -= self.lr * dW[l]
          b[l] -= self.lr * db[l]
          self.parameters['W' + str(l)] = W[l]
          self.parameters['b' + str(l)] = b[l]


    def get_batch(self, X, Y):

        np.random.seed(1)
        m = X.shape[1]
        mini_batches = []

        #shuffling the data
        permutation = list(np.random.permutation(X.shape[1]))
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]
        num_of_complete_batches = m // self.batch_size
        for i in range(num_of_complete_batches):
            mini_batch_X = X_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch_Y = Y_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)

        #if there is incomplete batch
        if m % self.batch_size!= 0:

            mini_batch_X=X_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch_Y=Y_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    def fit(self,
            X,
            Y,
            validation_data=None,
            epochs=100,
            patience=None):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values
        if isinstance(Y, (pd.core.frame.DataFrame,pd.core.series.Series)):
            Y = Y.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        if Y.ndim > 1:
            Y = Y.T
        else:
            Y = np.expand_dims(Y, axis=0)

        self.full_Y = Y
        self.full_X = X

        mini_batches = self.get_batch(X, Y)

        if validation_data:
            X_val, Y_val = validation_data

            if isinstance(X_val, pd.core.frame.DataFrame):
                X_val = X_val.values
            if isinstance(Y_val, (pd.core.frame.DataFrame,pd.core.series.Series)):
                Y_val = Y_val.values

            if X_val.ndim > 1:
                X_val = X_val.T
            else:
                X_val = np.expand_dims(X_val, axis=1)

            if Y_val.ndim > 1:
                Y_val = Y_val.T
            else:
                Y_val = np.expand_dims(Y_val, axis=0)

        if patience:
            epoch_cache ={}
        cost_train=[]
        cost_vali=[]
        for i in range(0, epochs):

            for mini_batch in mini_batches:

                mini_batch_X, mini_batch_Y = mini_batch

                self.X = mini_batch_X
                self.Y = mini_batch_Y

                AL = self.forward(mini_batch_X)
                cost = self.loss(AL, mini_batch_Y)
                train_acc = self.accuracy(AL, mini_batch_Y)



                self.backward()

            if validation_data:
                AL_val = self.forward(X_val)
                cost_val = self.loss(AL_val, Y_val)

            # Print the cost and acc every epoch
            if validation_data:
                train_acc = self.accuracy(AL, mini_batch_Y)
                val_acc = self.accuracy(AL_val, Y_val)
                self.val_accuracy.append(val_acc)
                print(f"Epoch {i+1} - Train_Loss:{cost}  Val_Loss:{cost_val} Train_Acc:{train_acc} Val_Acc:{val_acc}")
                self.cost_train.append(cost)
                self.train_accuracy.append(train_acc)
                self.cost_vali.append(cost_val)

            else:

                self.cost_train.append(cost)
                print(f"Epoch {i+1} - Train_Loss:{cost}")

            if patience:
               if not epoch_cache or cost_val <= epoch_cache['best_loss']:
                    epoch_cache['best_loss'] = cost_val
                    epoch_cache['best_parameters'] = self.parameters
                    counter=0
               elif cost_val > epoch_cache['best_loss']:
                    counter += 1

               if counter == patience:
                    self.parameters = epoch_cache['best_parameters']
                    print(f"Early Stopping! Best Epoch {i+1-patience} - Val_Loss:{epoch_cache['best_loss']}")
                    break


    def predict(self, X):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        AL = self.forward(X)

        if AL.shape[0]==1:
            y_pred = AL[0]>0.5
        else:
            y_pred = (AL == np.max(AL,axis=0)).astype(bool).T
        y_pred = y_pred*1
        y_pred. astype('float32')
        return y_pred


    def get_cost(self,validation=True):
      if validation:
          return self.cost_train,self.cost_vali
      else:
        return self.cost_train
    def score(self,validation=True):
      if validation:
        return self.train_accuracy, self.val_accuracy
      else:
        return self.train_accuracy

def evaluate_acc(y, y_pred):
  acc=np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))
  print('The evaluation accuracy is ', acc)

model3_tanh = Model([Layer(units=256,input_units=3072, activation='tanh'),
                Layer(units=256, activation='tanh'),

               Layer(units=10, activation='softmax')],batch_size=64,lr=0.01,max_norm=1,
               random_state=1)

model3_tanh.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),

          epochs=50
          )

"""## Task 3.3: Regularization

### Regularization L1
"""

class Layer:
    def __init__(self, units=None, input_units=None, activation=None, l1_regularization=0.01):
            self.input_units = input_units
            self.output_units = units
            self.regularization = l1_regularization
            assert activation in [None,'tanh','leaky_relu','relu','sigmoid','softmax'], 'Activation not recognized'
            self.activation = activation

class Model_reg:
    def __init__(self, layers, batch_size,lr,max_norm=None,random_state=21):

        np.random.seed(random_state)
        self.batch_size=batch_size
        self.lr=lr
        self.parameters = {}
        self.activations = {}
        self.gradients = {}
        self.max_norm=max_norm
        self.regularization ={}
        self.cache = {}
        self.X = None
        self.Y = None
        self.cost_train=[]
        self.cost_vali=[]
        self.train_accuracy = []
        self.val_accuracy = []

        self.L = len(layers)

        for l in range(1,self.L+1):

            layer = layers[l-1]

            if layer.input_units:
                input_units = layer.input_units
                output_units = layer.output_units
            else:
                input_units  = previous_output_units
                output_units = layer.output_units

            # Apply Xavier Initialization
            #xavier_stddev = np.sqrt(2 / (input_units + output_units))
            self.parameters['W' + str(l)] = np.random.randn(output_units,input_units) * np.sqrt(2./input_units)
            self.parameters['b' + str(l)] = np.zeros((layer.output_units,1))

            self.activations['a' + str(l)] = layer.activation
            self.regularization['r' + str(l)] = layer.regularization

            previous_output_units = layer.output_units

    def forward(self, X):
      A_prev = X
      for l in range(1, self.L+1):
        Wl = self.parameters['W' + str(l)]
        bl = self.parameters['b' + str(l)]
        activation_fn = self.activations['a' + str(l)]

        Zl = np.dot(Wl, A_prev) + bl
        if activation_fn == 'tanh':
            Al = np.tanh(Zl)
        elif activation_fn == 'relu':
            Al = np.maximum(0, Zl)
        elif activation_fn == 'sigmoid':
            Al = 1 / (1 + np.exp(-Zl))
        elif activation_fn == 'softmax':
            exp_Zl = np.exp(Zl)
            Al = exp_Zl / np.sum(exp_Zl, axis=0, keepdims=True)
        elif activation_fn == 'leaky_relu':
            Al = np.maximum(0.01 * Zl, Zl)

        self.cache['Z' + str(l)] = Zl
        self.cache['A' + str(l)] = Al

        A_prev = Al

      return Al




    def loss(self, AL, Y):

        m = Y.shape[1] # number of examples
        regularization_term = 0
        for l in range(1,self.L+1):
            #regularization_term += self.regularization['r' + str(l)]*np.linalg.norm(self.parameters['W' + str(l)],ord='fro')
            regularization_term += self.regularization['r' + str(l)]*np.sum(np.abs(self.parameters['W' + str(l)]))

        regularization_term /= (2*m)


        loss_each_example = -np.sum(Y * np.log(AL),axis=1)
        all_losses = np.sum(loss_each_example)
        cost = all_losses/m + regularization_term  # compute cost

        return cost

    def accuracy(self, A, Y):
        # calculate accuracy here

        predictions = np.argmax(A, axis=0)
        labels = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == labels)
        return accuracy

    def backward(self):
      m = self.X.shape[1] # number of examples

      # Retrieve all weights, biases, activations, and regularization parameters
      W, b, a, r = {}, {}, {}, {}
      A, Z = {}, {}
      for l in range(1, self.L+1):
          W[l] = self.parameters['W' + str(l)]
          b[l] = self.parameters['b' + str(l)]
          a[l] = self.activations['a' + str(l)]
          r[l] = self.regularization['r' + str(l)]

          A[l] = self.cache['A' + str(l)]
          Z[l] = self.cache['Z' + str(l)]
      A[0] = self.X

      # Backward propagation: calculate dW1, db1, ..., dWL, dbL
      dW, db, dZ = {}, {}, {}
      for l in reversed(range(1, self.L+1)):
          if a[l] == 'sigmoid' or a[l] == 'softmax':
              dZ[l] = A[l] - self.Y
          elif a[l] == 'tanh':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (1 - np.square(Z[l])))
          elif a[l] == 'relu':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0))
          elif a[l] == 'leaky_relu':
            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0) + (Z[l] <= 0) * 0.01)
          #dW[l] = np.dot(dZ[l], A[l-1].T) / m  + r[l] * W[l] / m
          dW[l] = np.dot(dZ[l], A[l-1].T) / m  + r[l] * np.sign(W[l]) / m
          db[l] = np.sum(dZ[l], axis=1, keepdims=True) / m

      # Update weights and biases using gradient descent
      for l in range(1, self.L+1):
          W[l] -= self.lr * dW[l]
          b[l] -= self.lr * db[l]
          self.parameters['W' + str(l)] = W[l]
          self.parameters['b' + str(l)] = b[l]


    def get_batch(self, X, Y):

        np.random.seed(1)
        m = X.shape[1]
        mini_batches = []

        #shuffling the data
        permutation = list(np.random.permutation(X.shape[1]))
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]
        num_of_complete_batches = m // self.batch_size
        for i in range(num_of_complete_batches):
            mini_batch_X = X_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch_Y = Y_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)

        #if there is incomplete batch
        if m % self.batch_size!= 0:

            mini_batch_X=X_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch_Y=Y_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    def fit(self,
            X,
            Y,
            validation_data=None,
            epochs=100,
            patience=None):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values
        if isinstance(Y, (pd.core.frame.DataFrame,pd.core.series.Series)):
            Y = Y.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        if Y.ndim > 1:
            Y = Y.T
        else:
            Y = np.expand_dims(Y, axis=0)

        self.full_Y = Y
        self.full_X = X

        mini_batches = self.get_batch(X, Y)

        if validation_data:
            X_val, Y_val = validation_data

            if isinstance(X_val, pd.core.frame.DataFrame):
                X_val = X_val.values
            if isinstance(Y_val, (pd.core.frame.DataFrame,pd.core.series.Series)):
                Y_val = Y_val.values

            if X_val.ndim > 1:
                X_val = X_val.T
            else:
                X_val = np.expand_dims(X_val, axis=1)

            if Y_val.ndim > 1:
                Y_val = Y_val.T
            else:
                Y_val = np.expand_dims(Y_val, axis=0)

        if patience:
            epoch_cache ={}
        cost_train=[]
        cost_vali=[]
        for i in range(0, epochs):

            for mini_batch in mini_batches:

                mini_batch_X, mini_batch_Y = mini_batch

                self.X = mini_batch_X
                self.Y = mini_batch_Y

                AL = self.forward(mini_batch_X)
                cost = self.loss(AL, mini_batch_Y)
                train_acc = self.accuracy(AL, mini_batch_Y)



                self.backward()

            if validation_data:
                AL_val = self.forward(X_val)
                cost_val = self.loss(AL_val, Y_val)

            # Print the cost and acc every epoch
            if validation_data:
                train_acc = self.accuracy(AL, mini_batch_Y)
                val_acc = self.accuracy(AL_val, Y_val)
                self.val_accuracy.append(val_acc)
                print(f"Epoch {i+1} - Train_Loss:{cost}  Val_Loss:{cost_val} Train_Acc:{train_acc} Val_Acc:{val_acc}")
                self.cost_train.append(cost)
                self.train_accuracy.append(train_acc)
                self.cost_vali.append(cost_val)

            else:

                self.cost_train.append(cost)
                print(f"Epoch {i+1} - Train_Loss:{cost}")

            if patience:
               if not epoch_cache or cost_val <= epoch_cache['best_loss']:
                    epoch_cache['best_loss'] = cost_val
                    epoch_cache['best_parameters'] = self.parameters
                    counter=0
               elif cost_val > epoch_cache['best_loss']:
                    counter += 1

               if counter == patience:
                    self.parameters = epoch_cache['best_parameters']
                    print(f"Early Stopping! Best Epoch {i+1-patience} - Val_Loss:{epoch_cache['best_loss']}")
                    break


    def predict(self, X):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        AL = self.forward(X)

        if AL.shape[0]==1:
            y_pred = AL[0]>0.5
        else:
            y_pred = (AL == np.max(AL,axis=0)).astype(bool).T
        y_pred = y_pred*1
        y_pred. astype('float32')
        return y_pred


    def get_cost(self,validation=True):
      if validation:
          return self.cost_train,self.cost_vali
      else:
        return self.cost_train
    def score(self,validation=True):
      if validation:
        return self.train_accuracy, self.val_accuracy
      else:
        return self.train_accuracy

def evaluate_acc(y, y_pred):
  acc=np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))
  print('The evaluation accuracy is ', acc)

"""lamda= 0.01"""

paramters=[Layer(units=256,input_units=3072, activation='relu',l1_regularization=0.01),
               Layer(units=256, activation='relu',l1_regularization=0.01),
               Layer(units=10, activation='softmax',l1_regularization=0.01)]
model_reg = Model_reg(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_reg.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30
    )

pred = np.where(model_reg.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

plt.plot(model_reg.score()[0],label='Training Loss')
plt.plot(model_reg.score()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model l1 regularized')
plt.legend()
plt.show()

plt.plot(model_reg.get_cost()[0],label='Training Loss')
plt.plot(model_reg.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model l1 regularized')
plt.legend()
plt.show()

"""lambda= 0.05  gives the worse accuracy"""

paramters=[Layer(units=256,input_units=3072, activation='relu',l1_regularization=0.05),
               Layer(units=256, activation='relu',l1_regularization=0.05),
               Layer(units=10, activation='softmax',l1_regularization=0.05)]
model_reg11 = Model_reg(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_reg11.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=10,

          epochs=20
    )

pred = np.where(model_reg11.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

"""lambda= 0.1  also gives the worsen accuracy"""

paramters=[Layer(units=256,input_units=3072, activation='relu',l1_regularization=0.1),
               Layer(units=256, activation='relu',l1_regularization=0.1),
               Layer(units=10, activation='softmax',l1_regularization=0.1)]
model_reg12 = Model_reg(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_reg12.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=10,

          epochs=20
    )

pred = np.where(model_reg12.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

"""### Regularization L2

Regularization L2

We used different lambdas to investigate which one would give the best result since grid search is highly time consuming and we found that 0.01 is the best.
"""

class Layer:
    def __init__(self, units=None, input_units=None, activation=None, l2_regularization=0):
            self.input_units = input_units
            self.output_units = units
            self.regularization = l2_regularization
            assert activation in [None,'tanh','leaky_relu','relu','sigmoid','softmax'], 'Activation not recognized'
            self.activation = activation

class Model_regl2:
    def __init__(self, layers, batch_size,lr,max_norm=None,random_state=21):

        np.random.seed(random_state)
        self.batch_size=batch_size
        self.lr=lr
        self.parameters = {}
        self.activations = {}
        self.gradients = {}
        self.max_norm=max_norm
        self.regularization ={}
        self.cache = {}
        self.X = None
        self.Y = None
        self.cost_train=[]
        self.cost_vali=[]
        self.train_accuracy = []
        self.val_accuracy = []

        self.L = len(layers)

        for l in range(1,self.L+1):

            layer = layers[l-1]

            if layer.input_units:
                input_units = layer.input_units
                output_units = layer.output_units
            else:
                input_units  = previous_output_units
                output_units = layer.output_units

            # Apply Xavier Initialization
            #xavier_stddev = np.sqrt(2 / (input_units + output_units))
            self.parameters['W' + str(l)] = np.random.randn(output_units,input_units) * np.sqrt(2./input_units)
            self.parameters['b' + str(l)] = np.zeros((layer.output_units,1))

            self.activations['a' + str(l)] = layer.activation
            self.regularization['r' + str(l)] = layer.regularization

            previous_output_units = layer.output_units

    def forward(self, X):
      A_prev = X
      for l in range(1, self.L+1):
        Wl = self.parameters['W' + str(l)]
        bl = self.parameters['b' + str(l)]
        activation_fn = self.activations['a' + str(l)]

        Zl = np.dot(Wl, A_prev) + bl
        if activation_fn == 'tanh':
            Al = np.tanh(Zl)
        elif activation_fn == 'relu':
            Al = np.maximum(0, Zl)
        elif activation_fn == 'sigmoid':
            Al = 1 / (1 + np.exp(-Zl))
        elif activation_fn == 'softmax':
            exp_Zl = np.exp(Zl)
            Al = exp_Zl / np.sum(exp_Zl, axis=0, keepdims=True)
        elif activation_fn == 'leaky_relu':
            Al = np.maximum(0.01 * Zl, Zl)

        self.cache['Z' + str(l)] = Zl
        self.cache['A' + str(l)] = Al

        A_prev = Al

      return Al




    def loss(self, AL, Y):

        m = Y.shape[1] # number of examples
        regularization_term = 0
        for l in range(1,self.L+1):
            #regularization_term += self.regularization['r' + str(l)]*np.linalg.norm(self.parameters['W' + str(l)],ord='fro')**2
            regularization_term += self.regularization['r' + str(l)]*np.sum(np.square(self.parameters['W' + str(l)]))

        regularization_term /= (2*m)


        loss_each_example = -np.sum(Y * np.log(AL),axis=1)
        all_losses = np.sum(loss_each_example)
        cost = all_losses/m + regularization_term  # compute cost

        return cost

    def accuracy(self, A, Y):
        # calculate accuracy here

        predictions = np.argmax(A, axis=0)
        labels = np.argmax(Y, axis=0)
        accuracy = np.mean(predictions == labels)
        return accuracy

    def backward(self):
      m = self.X.shape[1] # number of examples

      # Retrieve all weights, biases, activations, and regularization parameters
      W, b, a, r = {}, {}, {}, {}
      A, Z = {}, {}
      for l in range(1, self.L+1):
          W[l] = self.parameters['W' + str(l)]
          b[l] = self.parameters['b' + str(l)]
          a[l] = self.activations['a' + str(l)]
          r[l] = self.regularization['r' + str(l)]

          A[l] = self.cache['A' + str(l)]
          Z[l] = self.cache['Z' + str(l)]
      A[0] = self.X

      # Backward propagation: calculate dW1, db1, ..., dWL, dbL
      dW, db, dZ = {}, {}, {}
      for l in reversed(range(1, self.L+1)):
          if a[l] == 'sigmoid' or a[l] == 'softmax':
              dZ[l] = A[l] - self.Y
          elif a[l] == 'tanh':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (1 - np.square(Z[l])))
          elif a[l] == 'relu':
              dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0))
          elif a[l] == 'leaky_relu':
            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), (Z[l] > 0) + (Z[l] <= 0) * 0.01)
          dW[l] = np.dot(dZ[l], A[l-1].T) / m  + r[l] * W[l] / m
          db[l] = np.sum(dZ[l], axis=1, keepdims=True) / m


      # Update weights and biases using gradient descent
      for l in range(1, self.L+1):
          W[l] -= self.lr * dW[l]
          b[l] -= self.lr * db[l]
          self.parameters['W' + str(l)] = W[l]
          self.parameters['b' + str(l)] = b[l]


    def get_batch(self, X, Y):

        np.random.seed(1)
        m = X.shape[1]
        mini_batches = []

        #shuffling the data
        permutation = list(np.random.permutation(X.shape[1]))
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]
        num_of_complete_batches = m // self.batch_size
        for i in range(num_of_complete_batches):
            mini_batch_X = X_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch_Y = Y_shuffled[:,i* self.batch_size:(i+1)* self.batch_size]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)

        #if there is incomplete batch
        if m % self.batch_size!= 0:

            mini_batch_X=X_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch_Y=Y_shuffled[:,num_of_complete_batches* self.batch_size:num_of_complete_batches* self.batch_size + (m - self.batch_size *num_of_complete_batches)]
            mini_batch = (mini_batch_X,mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    def fit(self,
            X,
            Y,
            validation_data=None,
            epochs=100,
            patience=None):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values
        if isinstance(Y, (pd.core.frame.DataFrame,pd.core.series.Series)):
            Y = Y.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        if Y.ndim > 1:
            Y = Y.T
        else:
            Y = np.expand_dims(Y, axis=0)

        self.full_Y = Y
        self.full_X = X

        mini_batches = self.get_batch(X, Y)

        if validation_data:
            X_val, Y_val = validation_data

            if isinstance(X_val, pd.core.frame.DataFrame):
                X_val = X_val.values
            if isinstance(Y_val, (pd.core.frame.DataFrame,pd.core.series.Series)):
                Y_val = Y_val.values

            if X_val.ndim > 1:
                X_val = X_val.T
            else:
                X_val = np.expand_dims(X_val, axis=1)

            if Y_val.ndim > 1:
                Y_val = Y_val.T
            else:
                Y_val = np.expand_dims(Y_val, axis=0)

        if patience:
            epoch_cache ={}
        cost_train=[]
        cost_vali=[]
        for i in range(0, epochs):

            for mini_batch in mini_batches:

                mini_batch_X, mini_batch_Y = mini_batch

                self.X = mini_batch_X
                self.Y = mini_batch_Y

                AL = self.forward(mini_batch_X)
                cost = self.loss(AL, mini_batch_Y)
                train_acc = self.accuracy(AL, mini_batch_Y)



                self.backward()

            if validation_data:
                AL_val = self.forward(X_val)
                cost_val = self.loss(AL_val, Y_val)

            # Print the cost and acc every epoch
            if validation_data:
                train_acc = self.accuracy(AL, mini_batch_Y)
                val_acc = self.accuracy(AL_val, Y_val)
                self.val_accuracy.append(val_acc)
                print(f"Epoch {i+1} - Train_Loss:{cost}  Val_Loss:{cost_val} Train_Acc:{train_acc} Val_Acc:{val_acc}")
                self.cost_train.append(cost)
                self.train_accuracy.append(train_acc)
                self.cost_vali.append(cost_val)

            else:

                self.cost_train.append(cost)
                print(f"Epoch {i+1} - Train_Loss:{cost}")

            if patience:
               if not epoch_cache or cost_val <= epoch_cache['best_loss']:
                    epoch_cache['best_loss'] = cost_val
                    epoch_cache['best_parameters'] = self.parameters
                    counter=0
               elif cost_val > epoch_cache['best_loss']:
                    counter += 1

               if counter == patience:
                    self.parameters = epoch_cache['best_parameters']
                    print(f"Early Stopping! Best Epoch {i+1-patience} - Val_Loss:{epoch_cache['best_loss']}")
                    break


    def predict(self, X):

        if isinstance(X, pd.core.frame.DataFrame):
            X = X.values

        if X.ndim > 1:
            X = X.T
        else:
            X = np.expand_dims(X, axis=1)

        AL = self.forward(X)

        if AL.shape[0]==1:
            y_pred = AL[0]>0.5
        else:
            y_pred = (AL == np.max(AL,axis=0)).astype(bool).T
        y_pred = y_pred*1
        y_pred. astype('float32')
        return y_pred


    def get_cost(self,validation=True):
      if validation:
          return self.cost_train,self.cost_vali
      else:
        return self.cost_train
    def score(self,validation=True):
      if validation:
        return self.train_accuracy, self.val_accuracy
      else:
        return self.train_accuracy

def evaluate_acc(y, y_pred):
  acc=np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))
  print('The evaluation accuracy is ', acc)



paramters=[Layer(units=256,input_units=3072, activation='relu',l2_regularization=0.01),
               Layer(units=256, activation='relu',l2_regularization=0.01),
               Layer(units=10, activation='softmax')]
model_regl2 = Model_regl2(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_regl2.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30)

pred = np.where(model_regl2.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

plt.plot(model_regl2.score()[0],label='Training acc')
plt.plot(model_regl2.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the third model l2 regularized')

plt.legend()
plt.show()

plt.plot(model_regl2.get_cost()[0],label='Training Loss')
plt.plot(model_regl2.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model l2regularized')
plt.legend()
plt.show()

paramters=[Layer(units=256,input_units=3072, activation='relu',l2_regularization=0.01),
               Layer(units=256, activation='relu',l2_regularization=0.01),
               Layer(units=10, activation='softmax')]
model_regl2 = Model_regl2(paramters, batch_size=64,lr=0.01,
                random_state=0)

model_regl2.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30)

pred = np.where(model_regl2.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

y_pred=model_regl2.predict(X_test)

evaluate_acc(y_test,y_pred)

plt.plot(model_regl2.score()[0],label='Training acc')
plt.plot(model_regl2.score()[1],label='Validation acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy for the third model l2regularized')

plt.legend()
plt.show()

plt.plot(model_regl2.get_cost()[0],label='Training Loss')
plt.plot(model_regl2.get_cost()[1],label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss for the third model l2regularized')
plt.legend()
plt.show()

"""Lambda 0.05 for L2"""

paramters=[Layer(units=256,input_units=3072, activation='relu',l2_regularization=0.05),
               Layer(units=256, activation='relu',l2_regularization=0.05),
               Layer(units=10, activation='softmax')]
model_regl2 = Model_regl2(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_regl2.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30)

pred = np.where(model_regl2.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

"""Lmbda 0.1 show the worse results"""

paramters=[Layer(units=256,input_units=3072, activation='relu',l2_regularization=0.1),
               Layer(units=256, activation='relu',l2_regularization=0.1),
               Layer(units=10, activation='softmax')]
model_regl2 = Model_regl2(paramters, batch_size=64,lr=0.01,
                random_state=4)

model_regl2.fit(X_train,
          y_train,
          validation_data=(X_val,y_val),patience=5,

          epochs=30)

pred = np.where(model_regl2.predict(X_test)==1)[1]
test = np.where(y_test==1)[1]
print(classification_report(test,pred,digits=4))

"""## Task 3.4: Non normalized data"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)

X_train_nonorm = np.reshape(x_train,(50000,3072))
X_test_nonorm = np.reshape(x_test,(10000,3072))
X_train_nonorm = X_train_nonorm.astype('float32')
X_test_nonorm = X_test_nonorm.astype('float32')

X_train_no, X_val_no, y_train, y_val = train_test_split(X_train_nonorm,y_train,random_state=42,stratify=y_train)

twohidden_layer_parameters=[Layer(units=256,input_units=3072, activation='relu'),
               Layer(units=256, activation='relu'),
               Layer(units=10, activation='softmax')]
model3 = Model(twohidden_layer_parameters,  batch_size=128,lr=0.01,
               random_state=10)

model3.fit(X_train_no,
          y_train,
          validation_data=(X_val_no,y_val),patience=10,

          epochs=100
          )

"""## Task 3.5: CNN"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

model = models.Sequential()
model.add(layers.Conv2D(256, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(10))

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(x_train,y_train, epochs=10,
                    validation_data=(x_test,y_test))

"""Result for model with 256 unit layer"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy CNN')

plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)

"""Result for model with 32 unit layer"""

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(10))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(x_train,y_train, epochs=10,
                    validation_data=(x_test,y_test))

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy CNN')

plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)

"""## Task 3.6: CNN pretrained

### ResNet 50
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm

# Define the transforms for the data
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load the CIFAR-10 data
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=128,
                                         shuffle=False, num_workers=2)

# Load the pre-trained ResNet50 model
resnet50 = torchvision.models.resnet50(pretrained=True)

# Modify the output layer for CIFAR-10 classification
num_ftrs = resnet50.fc.in_features
resnet50.fc = nn.Linear(num_ftrs, 10)

# Move the model to GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
resnet50.to(device)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)

# Train the model
train_loss = []
train_acc = []
test_loss = []
test_acc = []

for epoch in range(10):
    running_loss = 0.0
    correct = 0
    total = 0
    resnet50.train()
    for data in tqdm(trainloader, desc=f'Epoch {epoch + 1}/{10}'):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = resnet50(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss_train = running_loss / len(trainloader)
    epoch_acc_train = correct / total
    train_loss.append(epoch_loss_train)
    train_acc.append(epoch_acc_train)

    running_loss = 0.0
    correct = 0
    total = 0
    resnet50.eval()
    with torch.no_grad():
        for data in tqdm(testloader, desc=f'Test Epoch {epoch + 1}/{10}'):
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = resnet50(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / len(testloader)
        epoch_acc = correct / total
        test_loss.append(epoch_loss)
        test_acc.append(epoch_acc)

    print(f'Epoch {epoch+1}/{10} - '
          f'Train Loss: {epoch_loss_train:.4f} - Train Acc: {epoch_acc_train:.4f} - '
          f'Test Loss: {epoch_loss:.4f} - Test Acc: {epoch_acc:.4f}')

# Plot the learning curves
plt.plot(train_loss, label='Training loss')
plt.plot(test_loss, label='Validation loss')
plt.legend(frameon=False)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train/val loss ResNet 50')
plt.show()

plt.plot(train_acc, label='Training accuracy')
plt.plot(test_acc, label='Validation accuracy')
plt.legend(frameon=False)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train/val accuracy Resnet50')

plt.show()

"""### DenseNet 201"""

import tensorflow as tf
import tensorflow.keras as K
import numpy as np
import matplotlib.pyplot as plt

def preprocess_data(X, Y):
    X = K.applications.densenet.preprocess_input(X)
    Y = K.utils.to_categorical(Y)
    return X, Y

# load the Cifar10 dataset
(x_train, y_train), (x_test, y_test) = K.datasets.cifar10.load_data()
# preprocess the data
x_train, y_train = preprocess_data(x_train, y_train)
x_test, y_test = preprocess_data(x_test, y_test)

# initialized weights
initializer = K.initializers.he_normal()
input_tensor = K.Input(shape=(32, 32, 3))
# resize images to pretrained size
resized_images = K.layers.Lambda(lambda image: tf.image.resize(image, (224, 224)))(input_tensor)
model = K.applications.DenseNet201(include_top=False,
                                   weights='imagenet',
                                   input_tensor=resized_images,
                                   input_shape=(224, 224, 3),
                                   pooling='max',
                                   classes=1000)
# make the weights and biases of the model non-trainable by "freezing" each layer of the DenseNet201 network
for layer in model.layers:
    layer.trainable = False
output = model.layers[-1].output
# reshape the output feature map
flatten = K.layers.Flatten()
output = flatten(output)
layer_256 = K.layers.Dense(units=256,
                           activation='relu',
                           kernel_initializer=initializer,
                           kernel_regularizer=K.regularizers.l2())
output = layer_256(output)
dropout = K.layers.Dropout(0.5)
output = dropout(output)
softmax = K.layers.Dense(units=10,
                         activation='softmax',
                         kernel_initializer=initializer,
                         kernel_regularizer=K.regularizers.l2())
output = softmax(output)
model = K.models.Model(inputs=input_tensor, outputs=output)

model.compile(
         optimizer=K.optimizers.Adam(learning_rate=1e-4),
         loss='categorical_crossentropy',
         metrics=['accuracy'])

# reduce learning rate when val_accuracy has stopped improving
lr_reduce = K.callbacks.ReduceLROnPlateau(monitor='val_accuracy',
                                          factor=0.6,
                                          patience=2,
                                          verbose=1,
                                          mode='max',
                                          min_lr=1e-7)
# stop training when val_accuracy has stopped improving
early_stop = K.callbacks.EarlyStopping(monitor='val_accuracy',
                                       patience=3,
                                       verbose=1,
                                       mode='max')
# callback to save the Keras model and (best) weights (uploaded)
checkpoint = K.callbacks.ModelCheckpoint('cifar10.h5',
                                         monitor='val_accuracy',
                                         verbose=1,
                                         save_weights_only=False,
                                         save_best_only=True,
                                         mode='max',
                                         save_freq='epoch')

train_steps_per_epoch = x_train.shape[0] // 32
val_steps_per_epoch = x_test.shape[0] // 32
history = model.fit(x_train,y_train, batch_size=32,
                    steps_per_epoch=train_steps_per_epoch,
                    validation_data= [x_test, y_test],
                    epochs=20,
                    shuffle=True,
                    callbacks=[lr_reduce, early_stop, checkpoint],
                    verbose=1)

plt.figure(1, figsize = (15,8))
plt.subplot(221)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'])
plt.subplot(222)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'])
plt.show()

